[A Comparative Analysis of LLMs like BERT, BART, and T5](https://medium.com/@zaiinn440/a-comparative-analysis-of-llms-like-bert-bart-and-t5-a4a873251ff)

#BERT #T5 #LLM #introduction 

- BERT(Bidirectional Encoder Representations from Transformers): the model is trained to predict one word for the corresponding mask
- T5(Text-to-Text Transfer Transformer): hybrid, trained to output one word or multiple words for one mask